{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVJB0UvW4O4y"
      },
      "source": [
        "This notebook trains Llama 2 model with LoRA on the generated train split of SentEval datasets using different loss functions and tests their performance by training a Logistic Regression classifier with embeddings generated by the trained model and calculating the classification accuracy on the generated test split. **(In Progress)**\n",
        "\n",
        "- For this demo, we show the training and testing of Llama 2 using LoRA on the **Customer Reviews (CR)** dataset, using all three loss functions **CoSENT**, **In-Batch Negatives** and **Angle**.\n",
        "\n",
        "- The loss functions CoSENT, In-Batch Negatives and Angle are taken from <a href=\"https://github.com/SeanLee97/AnglE\">AnglE</a>, and the Cosine Similarity Loss is modified from <a href=\"https://sbert.net/docs/package_reference/sentence_transformer/losses.html#cosinesimilarityloss\">SBERT</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HauEi1vUr_l"
      },
      "source": [
        "## Library Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNFewUrtUaFg",
        "outputId": "f0f19f5d-7a26-4e71-b0f1-c5cbba2e41ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip install -q datasets accelerate peft transformers scikit-learn scipy -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWJYPqj-JTYq"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5OB8685hWHW"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Optional, Union, Any, Tuple\n",
        "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, AutoModelForCausalLM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.stats import spearmanr\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import gzip\n",
        "import csv\n",
        "from peft import (\n",
        "    get_peft_model, LoraConfig, TaskType, PeftModel,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from peft.tuners.lora import LoraLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "bYyZ-fo-UaFi",
        "outputId": "d77fa812-5c54-48ae-ce0a-929836d2d744"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NVIDIA RTX 6000 Ada Generation'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.get_device_name()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKd63ogoJZYp"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzXAVVK4UaFi"
      },
      "source": [
        "The Tokenizer method, for now the maximum length is fixed to 512, which is the maximum permissible token length limit for BERT models. This needs to be set to the length of the longest sentence in the batch...\n",
        "\n",
        "1. For **Classification Task**, there is only one column, `text1` containing the sentences and their corresponding `label` column. To work with the loss functions in this system, the sentences need to be tokenized and processed in pairs. Hence, the contents of the `text1` column needs to be duplicated into another column `text2` where an example will then contain 2 sentence columns (`text1` and `text2`) and a `label` column. For example,\n",
        "\n",
        "```python\n",
        "text1 = [\"Sentence1\", \"Sentence2\", \"Sentence3\", \"Sentence4\", \"Sentence5\"]\n",
        "label = [0, 1, 0, 1, 0]\n",
        "```\n",
        "\n",
        "needs to be converted into the following format before tokenization:\n",
        "\n",
        "```python\n",
        "text1 = [\"Sentence1\", \"Sentence2\", \"Sentence3\", \"Sentence4\", \"Sentence5\"]\n",
        "text2 = [\"Sentence1\", \"Sentence2\", \"Sentence3\", \"Sentence4\", \"Sentence5\"]\n",
        "label = [0, 1, 0, 1, 0]\n",
        "```\n",
        "\n",
        "where the `text2` column is just a duplicate of the column, `text1`\n",
        "\n",
        "2. In case of **STS task**, this step is not required, since it already contains a sentence pair and a label for each example. They are already in the following format:\n",
        "\n",
        "```python\n",
        "text1 = [\"Sentence1\", \"Sentence2\", \"Sentence3\", \"Sentence4\", \"Sentence5\"]\n",
        "text2 = [\"Sentence1*\", \"Sentence2*\", \"Sentence3*\", \"Sentence4*\", \"Sentence5*\"]\n",
        "label = [0.2, 1.2, 2.0, 3.8, 0.4]\n",
        "```\n",
        "where `\"Sentence1\"` and `\"Sentence1*\"` are the sentence pairs of the same example\n",
        "\n",
        "The output of the tokenizer will be the **tokens** for sentence pairs along with a `separate_id` demarcating `\"Sentence1\"` and `\"Sentence1*\"`.\n",
        "\n",
        "```python\n",
        "text1 = [\"Sentence1\", \"Sentence2\", ...]\n",
        "text2 = [\"Sentence1*\", \"Sentence2*\", ...]\n",
        "label = [0, ...]\n",
        "\n",
        "# The tokens will look like\n",
        "tokens = [\n",
        "  {\n",
        "    \"input_ids\": [102, 2019, 2093, 2910, 2, 0, 28823, 29371, 5738, 2],\n",
        "    \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    \"separate_ids\": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
        "    \"label\": [0]\n",
        "  }, # sentence_1_1*_token\n",
        "  {\n",
        "    \"input_ids\": [102, 2019, 2093, 2910, 2, 0, 28823, 29371, 5738, 2],\n",
        "    \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    \"separate_ids\": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
        "    \"label\": [1]\n",
        "  }, #sentence_2_2*_token\n",
        "  ...\n",
        "]\n",
        "```\n",
        "\n",
        "Hence, for each example, the token generated by the tokenizer will contain both the tokenized sentences and can be expressed as `sentence_1_1*_token`, `sentence_2_2*_token`, etc. which will be fed to the data collator, later in the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlya304lhWHX"
      },
      "outputs": [],
      "source": [
        "class CustomDataTokenizer:\n",
        "    def __init__(self, tokenizer, max_length = 512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, data: Dict) -> Dict:\n",
        "        text_columns = ['text1', 'text2']\n",
        "\n",
        "        tokens_list = []\n",
        "        for text_column in text_columns:\n",
        "            tokens_list.append(self.tokenizer(data[text_column], max_length=self.max_length, truncation=True))\n",
        "\n",
        "        token = {}\n",
        "        seperate_ids = []\n",
        "        for i, t in enumerate(tokens_list):\n",
        "            # Input IDs and Attention Masks are in \"t\"...\n",
        "            for key, val in t.items():\n",
        "                if i == 0:\n",
        "                    token[key] = val\n",
        "                else:\n",
        "                    token[key] += val\n",
        "                if key == 'input_ids':\n",
        "                    seperate_ids += [i] * len(val)\n",
        "\n",
        "        token['labels'] = [int(data['label']) if 'label' in data else -1]\n",
        "        token['seperate_ids'] = seperate_ids\n",
        "\n",
        "        return token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDR2QBJiKnBf"
      },
      "source": [
        "## Data Collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx8iSK42UaFl"
      },
      "source": [
        "The custom data collator processes the data passed on from the tokenizer, applies padding and processes the batches to be fed to the model for training. The list of tokenized inputs is converted into torch tensors in batches.\n",
        "\n",
        "The tokenized inputs containing tokenized sentence pairs for an example are expressed as `sentence_1_1*_token`, `sentence_2_2*_token`, etc. as discussed before.\n",
        "\n",
        "The collator performs the transformation of data from:\n",
        "\n",
        "```python\n",
        "text1 = [sentence_1_1*_token, sentence_2_2*_token, sentence_3_3*_token, sentence_4_4*_token, sentence_5_5*_token]\n",
        "labels = [0, 1, 2, 3, 0]\n",
        "```\n",
        "\n",
        "where `sentence_1_1*_token`, `sentence_2_2*_token`, ... are the combined tokenized outputs from the tokenizer containing both sentences of `text1` and `text2` for each example, demarcated by `separate_id`,\n",
        "\n",
        "To:\n",
        "\n",
        "```python\n",
        "text = [sentence_1_token, sentence_1*_token, sentence_2_token, sentence_2*_token,sentence_3_token, sentence_3*_token, sentence_4_token, sentence_4*_token, sentence_5_token, sentence_5*_token]\n",
        "labels = [0, 0, 1, 1, 2, 2, 4, 4, 0, 0]\n",
        "```\n",
        "\n",
        "where the sentences have been separated based on their `separate_id` values into separate samples, sharing the same labels which have been duplicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xar9ji1NhWHY"
      },
      "outputs": [],
      "source": [
        "# Modified from https://github.com/SeanLee97/AnglE/blob/main/angle_emb/angle.py#L568\n",
        "class CustomDataCollator:\n",
        "    tokenizer = None\n",
        "    padding = 'longest'\n",
        "    max_length: Optional[int] = 512\n",
        "    return_tensors: str = \"pt\"\n",
        "\n",
        "    def __init__(self, tokenizer_base, max_length):\n",
        "        self.tokenizer = tokenizer_base\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, features: List[Dict], return_tensors: str = \"pt\") -> Dict[str, torch.Tensor]:\n",
        "        if return_tensors is None:\n",
        "            return_tensors = self.return_tensors\n",
        "\n",
        "        # print(\"Unprocessed Features: \", features)\n",
        "        new_features = []\n",
        "        for feature in features:\n",
        "            seperate_ids = feature['seperate_ids']\n",
        "            input_ids = feature['input_ids']\n",
        "            attention_mask = feature['attention_mask']\n",
        "\n",
        "            max_seperate_id = max(seperate_ids)\n",
        "            prev_start_idx = 0\n",
        "            for seperate_id in range(1, max_seperate_id + 1):\n",
        "                start_idx = seperate_ids.index(seperate_id)\n",
        "\n",
        "                new_feature = {}\n",
        "                new_feature['input_ids'] = input_ids[prev_start_idx:start_idx]\n",
        "                new_feature['attention_mask'] = attention_mask[prev_start_idx:start_idx]\n",
        "                new_feature['labels'] = feature['labels']\n",
        "                new_features.append(new_feature)\n",
        "                prev_start_idx = start_idx\n",
        "\n",
        "            new_feature = {}\n",
        "            new_feature['input_ids'] = input_ids[prev_start_idx:]\n",
        "            new_feature['attention_mask'] = attention_mask[prev_start_idx:]\n",
        "            new_feature['labels'] = feature['labels']\n",
        "            new_features.append(new_feature)\n",
        "\n",
        "        del features\n",
        "        features = self.tokenizer.pad(\n",
        "            {'input_ids': [feature['input_ids'] for feature in new_features]},\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=return_tensors,\n",
        "        )\n",
        "        features['attention_mask'] = self.tokenizer.pad(\n",
        "            {'input_ids': [feature['attention_mask'] for feature in new_features]},\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=return_tensors,\n",
        "        )['input_ids']\n",
        "\n",
        "        features['labels'] = torch.Tensor([feature['labels'] for feature in new_features])\n",
        "        # print(\"Processed Features: \", features)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf43nyBfJpUi"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkSrRaOJUaFj"
      },
      "source": [
        "### 1. Default Pairwise Cosine Similarity Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgOxGN00TS0Y"
      },
      "source": [
        "The `default_cosine_similarity_loss` is a basic similarity-based loss function which calculates a loss value based on the cosine similarity of paired embeddings, and the respective ground truth labels. It normalizes the true similarity scores, computes cosine similarities between paired embeddings, and applies the mean squared error (MSE) function to produce a final loss value.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Pairwise Cosine Similarity Loss Equation**:\n",
        "\n",
        "$$\\mathcal{L}_{\\text{Cosine MSE}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( y_{\\text{true}} - \\cos(X_i, X_j) \\right)^2$$\n",
        "\n",
        "  - ${y_\\text{true}}$ - Labelled similarity scores ranging between 0 and 5, which after normalization vary between 0 and 1.\n",
        "  - ${X_i, X_j}$ - Embeddings of the corresponding sentences ${S_i}$ and ${S_j}$ respectively.\n",
        "  - ${\\cos(X, Y)}$ - Cosine similarity between the sentence embeddings ${X}$ and ${Y}$.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Loss Function SBERT Reference**:\n",
        "<a href=\"https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#sentence_transformers.losses.CosineSimilarityLoss\"> Sentence Transformers/CosineSimilarityLoss</a>\n",
        "\n",
        "**GitHub Reference**:\n",
        "<a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/CosineSimilarityLoss.py#L10-L81\">Sentence Transformers/CosineSimilarityLoss</a>\n",
        "\n",
        "<br>\n",
        "\n",
        "**Implementation**:\n",
        "\n",
        "The true labels and predicted values are arranged in a paired manner such as `[x[0], x[1], x[2], x[3], ...]`, where `x[0]` and `x[1]` stand for a sentence pair. According to this illustration, the index of `x` is the index of an example, and the true labels are adjusted to fall between 0 and 1.\n",
        "\n",
        "This format, achieved by the data collator, is required for the dataset to accommodate the definition of the loss function. In the case of the STS task, pairs of sentences along with their corresponding similarity scores are provided.\n",
        "\n",
        "1. **Function Definition and Parameters**:\n",
        "  - **Parameters**:\n",
        "    - `y_true`: A tensor of ground truth labels in a specific paired style where each pair of true values is structured sequentially. This represents the similarity score corresponding to each sentence pair. Shape: `(batch_size, 1)`\n",
        "    - `y_pred`: A tensor of model predictions (embedding vectors) also in the mentioned paired style, where each pair of predicted values is structured sequentially. Shape: `(batch_size, 2 * embedding_vector_length)`, where `embedding_vector_length` depends on the model.\n",
        "    - `tau`: A scaling factor (default is 1.0), but not used in this specific implementation.\n",
        "  - **Returns**:\n",
        "    - A tensor representing the loss value.\n",
        "\n",
        "  - **Semantic Textual Similarity (STS) Task**:\n",
        "  The STS datasets consist of sentence pairs and their corresponding similarity scores. For example, the datasets are arranged in the following way:\n",
        "  ```python\n",
        "  text1 = [\"Sentence 1\", \"Sentence 2\", \"Sentence 3\", \"Sentence 4\", \"Sentence 5\"]\n",
        "  text2 = [\"Sentence 1*\", \"Sentence 2*\", \"Sentence 3*\", \"Sentence 4*\", \"Sentence 5*\"]\n",
        "  label = [0.2, 1.2, 2.0, 3.8, 0.4]\n",
        "  ```\n",
        "  \n",
        "  After tokenization, collation, and passing through the model, the `y_true` and `y_pred` will be transformed into the following format to accommodate the definition of the loss function:\n",
        "\n",
        "  ```python\n",
        "  y_true: [0.2, 1.2, 2.0, 3.8, 0.4]\n",
        "  y_pred: [emb_1, emb_1*, emb_2, emb_2*, emb_3, emb_3*, emb_4, emb_4*, emb_5, emb_5*]\n",
        "  ```\n",
        "\n",
        "2. **Normalizing `y_true`:**\n",
        "  ```python\n",
        "  y_true = y_true / 5.0\n",
        "  y_true = y_true[::2, 0]\n",
        "  ```\n",
        "  This normalizes the labelled similarity scores to a range of `0` to `1`, since the the labels vary between `0` and `5`. It also selects every second element from `y_true` starting from the first element, since that is how the data had been arranged before entry into the loss function, where the 0th and 1st labels will be same as they belong to the sentence 1 and sentence 2 of the same data point respectively sharing the same labels, and so on.\n",
        "\n",
        "3. **Splitting y_pred into Pairs**:\n",
        "  ```python\n",
        "  y_pred1 = y_pred[0::2]\n",
        "  y_pred2 = y_pred[1::2]\n",
        "  ```\n",
        "\n",
        "  `y_pred1` and `y_pred2` refer to the predicted embeddings of sentences ${S_1}$ and ${S_1^*}$ respectively of a data point or sentence pair.\n",
        "\n",
        "4. **Computing Cosine Similarity for Pairs**:\n",
        "  ```python\n",
        "  cos_sim = F.cosine_similarity(y_pred1, y_pred2)\n",
        "  ```\n",
        "\n",
        "  The cosine similarity between the pairs of embeddings is computed. This measures the similarity between two vectors of an inner product space which is determined by the cosine of the angle between the two vectors and determines whether they are pointing in the same direction.\n",
        "\n",
        "5. **Calculating MSE Loss**:\n",
        "  ```python\n",
        "  squared_difference = (y_true - cos_sim) ** 2\n",
        "  loss = squared_difference.mean()\n",
        "  ```\n",
        "\n",
        "  The mean squared error (MSE) loss is computed by taking the squared difference between the true similarity scores (`y_true`) and the calculated cosine similarities (`cos_sim`). The mean of these squared differences is then computed to get the final loss value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEImD1GKhtgQ"
      },
      "outputs": [],
      "source": [
        "# Modified from https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/CosineSimilarityLoss.py#L10-L81\n",
        "def default_cosine_similarity_loss(y_true, y_pred, tau=1):\n",
        "    # Normalizing y_true values to fall between 0 and 1...\n",
        "    y_true = y_true / 5.0\n",
        "    y_true = y_true[::2, 0]\n",
        "    y_pred1 = y_pred[0::2]\n",
        "    y_pred2 = y_pred[1::2]\n",
        "\n",
        "    # Calculating the cosine similarity between the pairs of embeddings...\n",
        "    cos_sim = F.cosine_similarity(y_pred1, y_pred2)\n",
        "\n",
        "    # MSE loss...\n",
        "    squared_difference = (y_true - cos_sim) ** 2\n",
        "    loss = squared_difference.mean()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gf8LPyIiW4Y"
      },
      "source": [
        "### 2. CoSENT Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE5v8WjEW208"
      },
      "source": [
        "The `cosent_loss` function also calculates a loss value based on the cosine similarity of paired embeddings, and depends on the respective ground truth labels for maintaining the relative ranking of the data points. It normalizes the embeddings, computes cosine similarities, adjusts these similarities based on the order of true labels, and applies the $\\text{log-sum-exp}$ function to produce a final loss value.\n",
        "\n",
        "<br>\n",
        "\n",
        "**CoSENT Loss Equation**:\n",
        "\n",
        "  $$ \\mathcal{L}_{\\text{CoSENT}} = \\log \\left[ 1 + \\sum_{s(X_i, X_j) > s(X_m, X_n)} e^{\\frac{\\cos(X_m, X_n) - \\cos(X_i, X_j)}{\\tau}} \\right] $$\n",
        "  \n",
        "  - ${X_i, X_j, X_m, X_n}$ - Embeddings of the corresponding sentences ${S_i, S_j, S_m}$ and ${S_n}$ respectively.\n",
        "  - ${s(X_i, X_j), s(X_m, X_n)}$ - Provided similarity scores between the sentence pairs ${(X_i, X_j)}$ and ${(X_m, X_n)}$ respectively.\n",
        "  - ${\\cos(X, Y)}$ - Cosine similarity between the sentence embeddings ${X}$ and ${Y}$.\n",
        "  - ${\\tau}$ - Temperature hyperparameter.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Reference**:\n",
        "    Huang, X., Peng, H., Zou, D., Liu, Z., Li, J., Liu, K., Wu, J., Su, J., & Yu, P. S. (2024). CoSENT: Consistent Sentence Embedding via Similarity Ranking. IEEE/ACM Transactions on Audio, Speech, and Language Processing. https://doi.org/10.1109/TASLP.2024.3402087\n",
        "\n",
        "<br>\n",
        "\n",
        "**Implementation**:\n",
        "<br>\n",
        "\n",
        "The true labels and predicted values are arranged in a \"zigzag\" manner such as `[x[0][0], x[0][1], x[1][0], x[1][1], ...]`, where `(x[0][0], x[0][1])` stands for a sentence pair. According to this illustration, the first index of `x` is the index of an example and the second index of `x` refers to `text1` denoted by '0' and `text2` denoted by 1.\n",
        "\n",
        "This format, achieved by the data collator, is required for the dataset to accommodate the definition of the loss function. In case of classification task, `x[0][0]` and `x[0][1]` refer to the same sentence (`text1` and `text2`), having the same labels, and in case of STS, `x[0][0]` stands for `text1` and `x[0][1]` stands for `text2`, also having the same label. This is explained with an example below.\n",
        "\n",
        "1. **Function Definition and Parameters**:\n",
        "   - **Parameters**:\n",
        "     - `y_true`: A tensor of ground truth labels in the mentioned specific \"zigzag\" style where each pair of true values is structured sequentially. This represents the label corresponding to each sentence. Shape: `(batch_size, 1)`\n",
        "     - `y_pred`: A tensor of model predictions (embedding vectors) also in the mentioned \"zigzag\" style, where each pair of predicted values is structured sequentially. Shape: `(batch_size, 2 * embedding_vector_length)`, where `embedding_vector_length` depends on the model. For example, for BERT Base, it is ${768}$ and ${1024}$ in case of BERT Large etc.\n",
        "     - `tau`: A scaling factor (default is 20.0) representing the temperature hyperparameter which controls the sharpness of the output distribution. A higher value of tau makes the output distribution sharper or makes the model more sensitive to differences between examples, whereas a lower value makes it smoother or less sensitive to differences.\n",
        "   - **Returns**:\n",
        "     - A tensor representing the loss value.\n",
        "\n",
        "  - **Classification Task**:\n",
        "  For example, in classification tasks, where there is a list of sentences and their corresponding binary labels:\n",
        "\n",
        "  ```python\n",
        "  text1 = [\"Sentence 1\", \"Sentence 2\", \"Sentence 3\", \"Sentence 4\", \"Sentence 5\"]\n",
        "  text2 = [\"Sentence 1\", \"Sentence 2\", \"Sentence 3\", \"Sentence 4\", \"Sentence 5\"]\n",
        "  label = [0, 1, 0, 1, 0]\n",
        "  ```\n",
        "\n",
        "  After tokenization, collation and passing through the model, the true labels (`y_true`) and predicted value (embedding vectors) (`y_pred`) will be transformed into duplicates in the following way to accommodate the definition of the loss function:\n",
        "\n",
        "  ```python\n",
        "  y_true: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
        "  y_pred: [emb_1, emb_1, emb_2, emb_2, emb_3, emb_3, emb_4, emb_4, emb_5, emb_5]\n",
        "  ```\n",
        "\n",
        "  - **STS Task**:\n",
        "  Such a transformation is not required for STS tasks since they are already sentence pairs (text1 and text2), along with their corresponding labels. Their arrangement is handled by the tokenizer and data collator which form the required zigzag pattern for training.\n",
        "\n",
        "  For Example, the dataset is arranged in the following way:\n",
        "  ```python\n",
        "  text1 = [\"Sentence 1\", \"Sentence 2\", \"Sentence 3\", \"Sentence 4\", \"Sentence 5\"]\n",
        "  text2 = [\"Sentence 1*\", \"Sentence 2*\", \"Sentence 3*\", \"Sentence 4*\", \"Sentence 5*\"]\n",
        "  label = [0.2, 1.2, 2.0, 3.8, 0.4]\n",
        "  ```\n",
        "\n",
        "  where `sentence1` (`text1`) and `sentence1*` (`text2`) are sentence pairs of an example with label, `o.2`\n",
        "\n",
        "  After tokenization, collation and passing through the model, the `y_true` and `y_pred` will be transformed into the following format to accomodate the definition of the loss function:\n",
        "\n",
        "  ```python\n",
        "  y_true: [0, 0, 1, 1, 2, 2, 4, 4, 0, 0]\n",
        "  y_pred: [emb_1, emb_1*, emb_2, emb_2*, emb_3, emb_3*, emb_4, emb_4*, emb_5, emb_5*]\n",
        "  ```\n",
        "\n",
        "2. **Reshaping `y_true`**:\n",
        "   ```python\n",
        "   y_true = y_true[::2, 0]\n",
        "   ```\n",
        "   - This line selects every second element from `y_true` starting from the first element and takes the first value from each pair. Essentially, it reduces the `y_true` tensor to half its length, focusing only on the first element of each pair.\n",
        "\n",
        "3. **Creating Pairwise Label Matrix**:\n",
        "   ```python\n",
        "   y_true = (y_true[:, None] < y_true[None, :]).float()\n",
        "   ```\n",
        "   - This creates a pairwise comparison matrix for the ground truth labels. For each pair `(i, j)`, it checks if `y_true[i]` is less than `y_true[j]`. The result is a binary matrix (0 or 1) where each element indicates the order relationship between pairs.\n",
        "\n",
        "4. **Normalizing `y_pred`**:\n",
        "   ```python\n",
        "   y_pred = F.normalize(y_pred, p=2, dim=1)\n",
        "   ```\n",
        "   - The predicted vectors (`y_pred`) are normalized to have unit length ($L^2$ norm).\n",
        "\n",
        "5. **Computing Cosine Similarity for Pairs**:\n",
        "   ```python\n",
        "   y_pred = torch.sum(y_pred[::2] * y_pred[1::2], dim=1) * tau\n",
        "   ```\n",
        "   - This computes the cosine similarity between the pairs of vectors in `y_pred`. For each pair `(i, i + 1)`, it multiplies the corresponding vectors element-wise, sums them up, and scales by the factor `tau`.\n",
        "\n",
        "6. **Creating Pairwise Score Differences**:\n",
        "   ```python\n",
        "   y_pred = y_pred[:, None] - y_pred[None, :]\n",
        "   ```\n",
        "   - This line computes the pairwise differences between the cosine similarity scores obtained in the previous step.\n",
        "\n",
        "7. **Adjusting Pairwise Differences with `y_true`**:\n",
        "   ```python\n",
        "   y_pred = (y_pred - (1 - y_true) * 1e12).view(-1)\n",
        "   ```\n",
        "   - The pairwise differences are adjusted by a large negative value (`-1e12`) for pairs that are not in the correct order according to `y_true`. This effectively masks out the incorrect pairs by making their differences very large and negative.\n",
        "\n",
        "8. **Adding Zero to `y_pred`**:\n",
        "   ```python\n",
        "   zero = torch.Tensor([0]).to(y_pred.device)\n",
        "   y_pred = torch.concat((zero, y_pred), dim=0)\n",
        "   ```\n",
        "   - A zero tensor is concatenated to `y_pred` to ensure numerical stability in the next step.\n",
        "\n",
        "  The $\\text{log-sum-exp}$ (LSE) function is used to compute a stable logarithm of the sum of exponentials, which is a common operation in various loss functions. The formula for the LSE is:\n",
        "\n",
        "  $$\\text{log-sum-exp}(x) = \\log \\left( \\sum_i e^{x_i} \\right)$$\n",
        "\n",
        "  This function is sensitive to large negative values in ${x_i}$, which can cause numerical instability or underflow issues. To prevent this, a zero element is added to the vector before applying the $\\text{log-sum-exp}$ operation which ensures that there is at least one element in the tensor that does not contribute to the instability.\n",
        "\n",
        "  - **Creating Zero Tensor**:\n",
        "    ```python\n",
        "    zero = torch.Tensor([0]).to(y_pred.device)\n",
        "    ```\n",
        "    - This line creates a tensor containing a single zero and moves it to the same device (CPU or GPU) as `y_pred`. This ensures that tensor operations are performed on the same hardware, avoiding device mismatch errors.\n",
        "\n",
        "  - **Concatenating Zero with `y_pred`**:\n",
        "    ```python\n",
        "    y_pred = torch.concat((zero, y_pred), dim=0)\n",
        "    ```\n",
        "    - This line concatenates the zero tensor to the beginning of `y_pred`. The result is a new tensor that has the zero element as its first element, followed by all elements of the original `y_pred`.\n",
        "\n",
        "  Effects of Zero Addition:\n",
        "\n",
        "  - **Avoiding Underflow**: By adding a zero, it is ensured that the $\\text{log-sum-exp}$ computation includes a stable baseline value. Since the exponential of zero is one ${e^0 = 1}$, it does not affect the sum in a significant way but prevents the entire sum from becoming too small (which can cause underflow).\n",
        "  - **Ensuring Positivity**: In certain cases, especially when all elements in `y_pred` are negative or very small, the sum of exponentials can become exceedingly small. Adding a zero ensures the sum remains positive and within a stable numerical range.\n",
        "\n",
        "9. **Computing the final Log-Sum-Exp loss value**:\n",
        "   ```python\n",
        "   return torch.logsumexp(y_pred, dim=0)\n",
        "   ```\n",
        "   - Finally, the $\\text{log-sum-exp}$ function is applied to `y_pred`. This operation is used to compute a smooth maximum and is commonly used in loss functions to ensure numerical stability and to handle a large range of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7eol13bhy0A"
      },
      "outputs": [],
      "source": [
        "# modified from: https://github.com/bojone/CoSENT/blob/124c368efc8a4b179469be99cb6e62e1f2949d39/cosent.py#L79\n",
        "def cosent_loss(y_true: torch.Tensor, y_pred: torch.Tensor, tau: float = 20.0) -> torch.Tensor:\n",
        "    # Input preparation...\n",
        "    y_true = y_true[::2, 0]\n",
        "    y_true = (y_true[:, None] < y_true[None, :]).float()\n",
        "\n",
        "    # Normalization of Logits...\n",
        "    y_pred = F.normalize(y_pred, p=2, dim=1)\n",
        "\n",
        "    # Cosine Similarity Calculation...\n",
        "    # y_pred[::2] and y_pred[1::2] select alternating embeddings, assuming they are paired...\n",
        "    # The dot product of these pairs gives the cosine similarity, scaled by a factor of tau to control the sharpness of similarity scores...\n",
        "    y_pred = torch.sum(y_pred[::2] * y_pred[1::2], dim=1) * tau\n",
        "\n",
        "    # Pairwise cosine similarity difference calculation...\n",
        "    y_pred = y_pred[:, None] - y_pred[None, :]\n",
        "\n",
        "    y_pred = (y_pred - (1 - y_true) * 1e12).view(-1)\n",
        "\n",
        "    zero = torch.Tensor([0]).to(y_pred.device)\n",
        "    y_pred = torch.concat((zero, y_pred), dim=0)\n",
        "    return torch.logsumexp(y_pred, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kVKFuNSh8ri"
      },
      "source": [
        "### 3. In-Batch Negatives Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FlQEvYk4ECF"
      },
      "source": [
        "The `in_batch_negative_loss` function function uses in-batch negatives which include the sentences not declared as positive pairs explicitly in a batch, to calculate the loss thus encouraging the model to learn embeddings such that positives are closer together and negatives are farther apart in the embedding space.\n",
        "\n",
        "<br>\n",
        "\n",
        "**In-Batch Negative Loss Equation**:\n",
        "\n",
        "$$ \\mathcal{L}_{\\text{ibn}} = - \\sum_{b} \\sum_{i}^{m} \\log \\left[ \\frac{e^{\\cos \\left( \\frac{X_{bi}, X_{bi}^+}{\\tau} \\right)}}{ \\sum_{j}^{N} e^{ \\cos \\left( \\frac{X_{bi}, X_{bj}^+}{\\tau} \\right)}} \\right]$$\n",
        "\n",
        "- ${b}$ - Batch number\n",
        "- ${X_{bi}}$ and ${X_{bj}}$ - Embeddings of the corresponding sentences ${S_{bi}}$ and ${S_{bj}}$ respectively.\n",
        "- ${X_{bi}^+}$ and ${X_{bj}^+}$ - Positive Samples of ${X_{bi}}$ and ${X_{bj}}$\n",
        "- ${m}$ - Number of Positive Pairs in ${b^{th}}$ batch\n",
        "- ${N}$ - Batch Size\n",
        "\n",
        "<br>\n",
        "\n",
        "**Reference**:\n",
        "    Tang, Y., Cheng, H., Fang, Y., & Pan, Y. (2022, October). In-Batch Negatives' Enhanced Self-Supervised Learning. In 2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI) (pp. 161-166). IEEE. https://doi.org/10.1109/ICTAI.2022.00029\n",
        "\n",
        "<br>\n",
        "\n",
        "**Implementation**:\n",
        "<br>\n",
        "\n",
        "1. **Function Definition and Parameters**:\n",
        "   - **Parameters**:\n",
        "     - `y_true`: A tensor of ground truth labels for the STS-Benchmark dataset, representing similarity scores for sentence pairs. Shape: `(batch_size, 1)`\n",
        "     - `y_pred`: A tensor of model predictions (embedding vectors) generated after passing through the BERT encoder layers. Shape: `(batch_size, 2 * embedding_vector_length)`\n",
        "     - `tau`: A scaling factor (default is 20.0) representing the temperature hyperparameter which controls the sharpness of the output distribution.\n",
        "     - `negative_weights`: A weight for negative samples, defaulting to 0.0.\n",
        "   - **Returns**:\n",
        "     - A tensor representing the loss value.\n",
        "\n",
        "  - **Example**:\n",
        "  For example, in the STS task, the dataset is arranged in the following way:\n",
        "  ```python\n",
        "  text1 = [\"Sentence 1\", \"Sentence 2\", \"Sentence 3\"]\n",
        "  text2 = [\"Sentence 1*\", \"Sentence 2*\", \"Sentence 3*\"]\n",
        "  label = [5, 1, 2]\n",
        "  ```\n",
        "\n",
        "  After tokenization, collation, and passing through the model, the true labels (`y_true`) and predicted value (embedding vectors) (`y_pred`) will be transformed into the following format to accommodate the definition of the loss function:\n",
        "\n",
        "  ```python\n",
        "  y_true: [5, 1, 2]\n",
        "  y_pred: [emb_1, emb_1*, emb_2, emb_2*, emb_3, emb_3*]\n",
        "  ```\n",
        "\n",
        "2. **Creating Target Matrix**:\n",
        "\n",
        "   ```python\n",
        "   def make_target_matrix(y_true: torch.Tensor):\n",
        "       idxs = torch.arange(0, y_pred.shape[0]).int().to(device)\n",
        "       y_true = y_true.int()\n",
        "       idxs_1 = idxs[None, :]\n",
        "       idxs_2 = (idxs + 1 - idxs % 2 * 2)[:, None]\n",
        "\n",
        "       idxs_1 *= y_true.T\n",
        "       idxs_1 += (y_true.T == 0).int() * -2\n",
        "\n",
        "       idxs_2 *= y_true\n",
        "       idxs_2 += (y_true == 0).int() * -1\n",
        "\n",
        "       y_true = (idxs_1 == idxs_2).float()\n",
        "       return y_true\n",
        "   ```\n",
        "\n",
        "   - This constructs a matrix to identify positive and negative pairs within the batch.\n",
        "     - **`idxs = torch.arange(0, y_pred.shape[0]).int().to(device)`**: Creates an array of indices for the batch.\n",
        "     - **`y_true = y_true.int()`**: Converts `y_true` to integer type.\n",
        "     - **`idxs_1 = idxs[None, :]`**: Expands `idxs` for broadcasting.\n",
        "     - **`idxs_2 = (idxs + 1 - idxs % 2 * 2)[:, None]`**: Creates alternating pairs of indices.\n",
        "     - **`idxs_1 *= y_true.T`**: Multiplies by transposed `y_true` to retain valid pairs.\n",
        "     - **`idxs_1 += (y_true.T == 0).int() * -2`**: Sets invalid pairs to -2.\n",
        "     - **`idxs_2 *= y_true`**: Multiplies by `y_true` to retain valid pairs.\n",
        "     - **`idxs_2 += (y_true == 0).int() * -1`**: Sets invalid pairs to -1.\n",
        "     - **`y_true = (idxs_1 == idxs_2).float()`**: Creates a binary target matrix where positive pairs are marked with 1 and negative pairs with 0.\n",
        "\n",
        "3. **Negative Mask**:\n",
        "\n",
        "   ```python\n",
        "   neg_mask = make_target_matrix(y_true == 0)\n",
        "   ```\n",
        "\n",
        "   - This creates a mask for identifying negative samples within the batch.\n",
        "     - **`make_target_matrix(y_true == 0)`**: Calls the `make_target_matrix` function with a condition to identify where the true labels are 0 (negative pairs).\n",
        "\n",
        "4. **Positive Samples Target Matrix**:\n",
        "\n",
        "   ```python\n",
        "   y_true = make_target_matrix(y_true)\n",
        "   ```\n",
        "\n",
        "   - This creates a target matrix for positive samples, representing the correct pairs within the batch.\n",
        "\n",
        "5. **Normalization and Similarity Calculation**:\n",
        "\n",
        "   ```python\n",
        "   y_pred = F.normalize(y_pred, dim=1, p=2)\n",
        "   similarities = y_pred @ y_pred.T\n",
        "   similarities = similarities - torch.eye(y_pred.shape[0]).to(device) * 1e12\n",
        "   similarities = similarities * tau\n",
        "   ```\n",
        "\n",
        "   - **Normalization**:\n",
        "     - **`y_pred = F.normalize(y_pred, dim=1, p=2)`**: Normalizes the embeddings to unit length (L2 norm) to ensure that the cosine similarity is valid by normalizing the embeddings.\n",
        "\n",
        "   - **Similarity Calculation**:\n",
        "     - **`similarities = y_pred @ y_pred.T`**: It computes the cosine similarity between all pairs of embeddings within the batch to calculate the similarities needed for the numerator and denominator in the loss equation.\n",
        "\n",
        "     $${\\cos \\left( X_{bi}, X_{bi}^+ \\right) \\text{,} \\cos \\left( X_{bi}, X_{bj}^+ \\right)}$$\n",
        "\n",
        "     - **Avoiding Self-Similarity**:\n",
        "       - **`similarities = similarities - torch.eye(y_pred.shape[0]).to(device) * 1e12`**: It subtracts a large value on the diagonal to avoid self-similarity to ensure that each embedding is not compared with itself.\n",
        "     - **Scaling**:\n",
        "       - **`similarities = similarities * tau`**: It scales the similarities by the temperature parameter ${\\tau}$ to adjust the sharpness of the distribution, making the model more sensitive to differences between examples.\n",
        "\n",
        "6. **Adjusting Similarities with Negative Weights**:\n",
        "\n",
        "   ```python\n",
        "   if negative_weights > 0:\n",
        "       similarities += neg_mask * negative_weights\n",
        "   ```\n",
        "\n",
        "   - This adjusts the similarities for negative samples if `negative_weights` is specified.\n",
        "     - **`similarities += neg_mask * negative_weights`**: Adds the negative mask weighted by `negative_weights` to the similarities.\n",
        "\n",
        "7. **Calculating Loss**:\n",
        "\n",
        "   The `categorical_crossentropy` function is implemented as:\n",
        "\n",
        "   ```python\n",
        "   def categorical_crossentropy(y_true, y_pred):\n",
        "       return -(F.log_softmax(y_pred, dim=1) * y_true).sum(dim=1)\n",
        "   ```\n",
        "   $${ \\log \\left[ \\frac{e^{\\cos \\left( X_{bi}, X_{bi}^+ \\right)}}{ \\sum_{j}^{N} e^{ \\cos \\left( X_{bi}, X_{bj}^+ \\right)}} \\right] }$$\n",
        "\n",
        "   - This function computes the loss value.\n",
        "\n",
        "8. **Calculating Mean Loss**:\n",
        "   \n",
        "   The mean loss is calculated by the `.mean()` after the `categorical_crossentropy` function.\n",
        "\n",
        "   ```python\n",
        "   return categorical_crossentropy(y_true, similarities).mean()\n",
        "   ```\n",
        "\n",
        "   $${- \\sum_{b} \\sum_{i}^{m}}$$\n",
        "\n",
        "   Where `m` represents the positive pairs in batch `b`.\n",
        "   - This calculates the mean loss value and returns the final loss for backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xxZKUVTh_Y5"
      },
      "outputs": [],
      "source": [
        "def categorical_crossentropy(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
        "    return -(F.log_softmax(y_pred, dim=1) * y_true).sum(dim=1)\n",
        "\n",
        "# Taken from https://github.com/SeanLee97/AnglE/blob/main/angle_emb/angle.py#L166\n",
        "def in_batch_negative_loss(y_true: torch.Tensor,\n",
        "                           y_pred: torch.Tensor,\n",
        "                           tau: float = 20.0,\n",
        "                           negative_weights: float = 0.0) -> torch.Tensor:\n",
        "    device = y_true.device\n",
        "\n",
        "    def make_target_matrix(y_true: torch.Tensor):\n",
        "        idxs = torch.arange(0, y_pred.shape[0]).int().to(device)\n",
        "        y_true = y_true.int()\n",
        "        idxs_1 = idxs[None, :]\n",
        "        idxs_2 = (idxs + 1 - idxs % 2 * 2)[:, None]\n",
        "\n",
        "        idxs_1 *= y_true.T\n",
        "        idxs_1 += (y_true.T == 0).int() * -2\n",
        "\n",
        "        idxs_2 *= y_true\n",
        "        idxs_2 += (y_true == 0).int() * -1\n",
        "\n",
        "        y_true = (idxs_1 == idxs_2).float()\n",
        "        return y_true\n",
        "\n",
        "    neg_mask = make_target_matrix(y_true == 0)\n",
        "\n",
        "    y_true = make_target_matrix(y_true)\n",
        "\n",
        "    y_pred = F.normalize(y_pred, dim=1, p=2)\n",
        "    similarities = y_pred @ y_pred.T\n",
        "    similarities = similarities - torch.eye(y_pred.shape[0]).to(device) * 1e12\n",
        "    similarities = similarities * tau\n",
        "\n",
        "    if negative_weights > 0:\n",
        "        similarities += neg_mask * negative_weights\n",
        "\n",
        "    return categorical_crossentropy(y_true, similarities).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzGhQzFHiEow"
      },
      "source": [
        "### 4. Angle Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHebP3UARo5c"
      },
      "source": [
        "The `angle_loss` function calculates the angle difference in complex space to address the saturation zone problem in cosine similarity, optimizing the model's ability to distinguish between similar and dissimilar samples effectively.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Angle Loss Equation**:\n",
        "\n",
        "$${\\mathcal{L}_{\\text{angle}} = \\log \\left[ 1 + \\sum_{s(X_i, X_j) > s(X_m, X_n)} e^{\\frac{\\Delta \\theta_{ij} - \\Delta \\theta_{mn}}{\\tau}} \\right]}$$\n",
        "\n",
        "- ${\\Delta \\theta_{ij}}$: Angle difference between embeddings ${X_i}$ and ${X_j}$\n",
        "- ${\\Delta \\theta_{mn}}$: Angle difference between embeddings ${X_m}$ and ${X_n}$\n",
        "- ${\\tau}$: Temperature hyperparameter\n",
        "\n",
        "<br>\n",
        "\n",
        "**Reference**:\n",
        "    Li, X., & Li, J. (2023). Angle-Optimized Text Embeddings. In Proceedings of the International Conference on Learning Representations (ICLR 2024). https://doi.org/10.48550/arXiv.2309.12871\n",
        "\n",
        "<br>\n",
        "\n",
        "**Implementation**:\n",
        "<br>\n",
        "\n",
        "1. **Function Definition and Parameters**:\n",
        "   - **Parameters**:\n",
        "     - `y_true`: Ground truth labels for the dataset, representing similarity scores for pairs. Shape: `(batch_size, 1)`\n",
        "     - `y_pred`: Model predictions (embedding vectors). Shape: `(batch_size, 2 * embedding_vector_length)`\n",
        "     - `tau`: Temperature hyperparameter, default is 1.0.\n",
        "   - **Returns**:\n",
        "     - A tensor representing the loss value.\n",
        "\n",
        "2. **Processing Ground Truth Labels**:\n",
        "   ```python\n",
        "   y_true = y_true[::2, 0]\n",
        "   ```\n",
        "   - This selects every second element from the ground truth tensor and prepares the ground truth labels for calculating pairwise comparisons.\n",
        "\n",
        "   ```python\n",
        "   y_true = (y_true[:, None] < y_true[None, :]).float()\n",
        "   ```\n",
        "   - It creates a matrix of pairwise comparisons, indicating which samples are less than others. It is a binary matrix for ${s(X_i, X_j) > s(X_m, X_n)}$ comparisons.\n",
        "\n",
        "3. **Splitting Predicted Embeddings into Real and Imaginary Parts**:\n",
        "   ```python\n",
        "   y_pred_re, y_pred_im = torch.chunk(y_pred, 2, dim=1)\n",
        "   ```\n",
        "   - This splits the predicted embeddings into real and imaginary parts by chunking which is dividing ${X_i}$ and ${X_j}$ into their real and imaginary components ${z = a + bi}$ and ${w = c + di}$.\n",
        "\n",
        "   ```python\n",
        "   a = y_pred_re[::2]\n",
        "   b = y_pred_im[::2]\n",
        "   c = y_pred_re[1::2]\n",
        "   d = y_pred_im[1::2]\n",
        "   ```\n",
        "   - It assigns the real and imaginary parts to variables corresponding to pairs thus preparing the variables for calculating the angle differences ${\\Delta \\theta_{ij}}$ and ${\\Delta \\theta_{mn}}$.\n",
        "\n",
        "4. **Calculating Angle Difference in Complex Space**:\n",
        "   ```python\n",
        "   z = torch.sum(c* *2 + d* *2, dim=1, keepdim=True)\n",
        "   ```\n",
        "   - It calculates the magnitude of the complex number for normalization which computes ${\\sqrt{c^2 + d^2}}$, the denominator in the angle difference calculation.\n",
        "\n",
        "   ```python\n",
        "   re = (a * c + b * d) / z\n",
        "   im = (b * c - a * d) / z\n",
        "   ```\n",
        "   - It calculates the real and imaginary parts of the normalized angle difference which implements the real and imaginary parts of the division ${\\frac{z}{w}}$ in the complex space.\n",
        "\n",
        "   ```python\n",
        "   dz = torch.sum(a**2 + b**2, dim=1, keepdim=True)**0.5\n",
        "   dw = torch.sum(c**2 + d**2, dim=1, keepdim=True)**0.5\n",
        "   ```\n",
        "   - This computes the magnitudes of the embeddings or ${|z|}$ and ${|w|}$ for normalization.\n",
        "\n",
        "   ```python\n",
        "   re /= (dz / dw)\n",
        "   im /= (dz / dw)\n",
        "   ```\n",
        "   - It normalizes the real and imaginary parts by their respective magnitudes. This is related to normalizing the angle difference to mitigate the impact of high variance of magnitudes, aligning with ${\\Delta \\theta_{ij}}$ and ${\\Delta \\theta_{mn}}$.\n",
        "\n",
        "5. **Combining and Adjusting Predictions**:\n",
        "   ```python\n",
        "   y_pred = torch.concat((re, im), dim=1)\n",
        "   ```\n",
        "   - It concatenates the real and imaginary parts thus combining the normalized real and imaginary parts into a single tensor.\n",
        "\n",
        "   ```python\n",
        "   y_pred = torch.abs(torch.sum(y_pred, dim=1)) * tau\n",
        "   ```\n",
        "   - It computes the absolute value of the sum of the angle differences, scaled by ${\\tau}$ which calculates the scaled angle differences, corresponding to ${\\frac{\\Delta \\theta_{ij} - \\Delta \\theta_{mn}}{\\tau}}$ in the loss equation.\n",
        "\n",
        "   ```python\n",
        "   y_pred = y_pred[:, None] - y_pred[None, :]\n",
        "   ```\n",
        "   - It computes the pairwise differences between angle differences to apply the exponential function in the loss equation.\n",
        "\n",
        "   ```python\n",
        "   y_pred = (y_pred - (1 - y_true) * 1e12).view(-1)\n",
        "   ```\n",
        "   - It adjusts the pairwise differences based on ground truth labels to emphasize correct pairs by appliying a large negative value to ensure that incorrect pairs do not dominate the loss calculation.\n",
        "\n",
        "6. **Final Loss Calculation**:\n",
        "   ```python\n",
        "   zero = torch.Tensor([0]).to(y_pred.device)\n",
        "   ```\n",
        "   - It initializes a zero tensor on the same device as the predictions to ensure a zero baseline for the ${\\text{log-sum-exp}}$ calculation.\n",
        "\n",
        "   ```python\n",
        "   y_pred = torch.concat((zero, y_pred), dim=0)\n",
        "   ```\n",
        "   - It concatenates the zero tensor with the predictions and prepares the tensor for the ${\\text{log-sum-exp}}$ operation.\n",
        "\n",
        "   ```python\n",
        "   return torch.logsumexp(y_pred, dim=0)\n",
        "   ```\n",
        "   - It computes the ${\\text{log-sum-exp}}$ of the adjusted predictions to get the final loss value:\n",
        "   $${\\log \\left[ 1 + \\sum e^{\\frac{\\Delta \\theta_{ij} - \\Delta \\theta_{mn}}{\\tau}} \\right]}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOw5jWY2hWHX"
      },
      "outputs": [],
      "source": [
        "# Taken from https://github.com/SeanLee97/AnglE/blob/main/angle_emb/angle.py#L117\n",
        "def angle_loss(y_true: torch.Tensor, y_pred: torch.Tensor, tau: float = 1.0):\n",
        "    y_true = y_true[::2, 0]\n",
        "    y_true = (y_true[:, None] < y_true[None, :]).float()\n",
        "\n",
        "    y_pred_re, y_pred_im = torch.chunk(y_pred, 2, dim=1)\n",
        "    a = y_pred_re[::2]\n",
        "    b = y_pred_im[::2]\n",
        "    c = y_pred_re[1::2]\n",
        "    d = y_pred_im[1::2]\n",
        "\n",
        "    z = torch.sum(c**2 + d**2, dim=1, keepdim=True)\n",
        "    re = (a * c + b * d) / z\n",
        "    im = (b * c - a * d) / z\n",
        "\n",
        "    dz = torch.sum(a**2 + b**2, dim=1, keepdim=True)**0.5\n",
        "    dw = torch.sum(c**2 + d**2, dim=1, keepdim=True)**0.5\n",
        "    re /= (dz / dw)\n",
        "    im /= (dz / dw)\n",
        "\n",
        "    y_pred = torch.concat((re, im), dim=1)\n",
        "    y_pred = torch.abs(torch.sum(y_pred, dim=1)) * tau\n",
        "    y_pred = y_pred[:, None] - y_pred[None, :]\n",
        "    y_pred = (y_pred - (1 - y_true) * 1e12).view(-1)\n",
        "    zero = torch.Tensor([0]).to(y_pred.device)\n",
        "    y_pred = torch.concat((zero, y_pred), dim=0)\n",
        "    return torch.logsumexp(y_pred, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Xwuv6RUaFk"
      },
      "source": [
        "### Combination of the Loss Functions with Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsvY39IuR7-7"
      },
      "source": [
        "The `TotalLoss` class combines three different loss functions, each contributing to the total loss based on their respective weights. This allows for adjustment of the importance of each loss component in the overall training objective.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Total Loss Equation**:\n",
        "\n",
        "$${\\mathcal{L}_{\\text{total}} = w_1 \\cdot \\mathcal{L}_{\\text{cosent}} + w_2 \\cdot \\mathcal{L}_{\\text{ibn}} + w_3 \\cdot \\mathcal{L}_{\\text{angle}}}$$\n",
        "\n",
        "- ${w_1}$: Weight for the CoSent loss\n",
        "- ${w_2}$: Weight for the in-batch negative loss\n",
        "- ${w_3}$: Weight for the angle loss\n",
        "\n",
        "<br>\n",
        "\n",
        "**Class Parameters**:\n",
        "  - `w1`: Weight for the CoSENT loss component, default is 1.0\n",
        "  - `w2`: Weight for the In-Batch Negatives loss component, default is 1.0\n",
        "  - `w3`: Weight for the Angle loss component, default is 1.0\n",
        "  - `cosent_tau`: Temperature parameter for the CoSENT loss, default is 20.0\n",
        "  - `ibn_tau`: Temperature parameter for the in-batch negative loss, default is 20.0\n",
        "  - `angle_tau`: Temperature parameter for the angle loss, default is 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adhkDi1bhWHX"
      },
      "outputs": [],
      "source": [
        "class TotalLoss:\n",
        "    def __init__(self,\n",
        "                w_cosent: float = 1.0,\n",
        "                w_ibn: float = 1.0,\n",
        "                w_angle: float = 1.0,\n",
        "                cosent_tau: float = 20.0,\n",
        "                ibn_tau: float = 20.0,\n",
        "                angle_tau: float = 1.0):\n",
        "        self.w_cosent = w_cosent\n",
        "        self.w_ibn = w_ibn\n",
        "        self.w_angle = w_angle\n",
        "        self.cosent_tau = cosent_tau\n",
        "        self.ibn_tau = ibn_tau\n",
        "        self.angle_tau = angle_tau\n",
        "\n",
        "    def __call__(self, labels: torch.Tensor, outputs: torch.Tensor) -> torch.Tensor:\n",
        "        loss = 0.\n",
        "        if (self.w_cosent == 0 and self.w_ibn == 0 and self.w_angle == 0):\n",
        "            loss += default_cosine_similarity_loss(labels, outputs)\n",
        "        if self.w_cosent > 0:\n",
        "            loss += self.w_cosent * cosent_loss(labels, outputs, self.cosent_tau)\n",
        "        if self.w_ibn > 0:\n",
        "            loss += self.w_ibn * in_batch_negative_loss(labels, outputs, self.ibn_tau)\n",
        "        if self.w_angle > 0:\n",
        "            loss += self.w_angle * angle_loss(labels, outputs, self.angle_tau)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXi45wR2KFFv"
      },
      "source": [
        "## Pooler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz8a3YGkUaFk"
      },
      "source": [
        "The `Pooler` class provides various strategies for pooling the output of the model, allowing options for how the hidden states are aggregated to form a single representation.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Pooling Strategies**:\n",
        "\n",
        "- `cls`: It uses the CLS token's representation in the last hidden state.\n",
        "- `cls_avg`: It averages the CLS token's representation with the mean of all tokens' representations.\n",
        "- `last`: It uses the representation of the last token.\n",
        "- `avg`: It averages the representations of all tokens, weighted by attention mask.\n",
        "- `max`: It uses the maximum value of the token representations, weighted by attention mask.\n",
        "- `all`: It returns the representations of all tokens.\n",
        "- `specific token index`: If an integer is passed as the pooling strategy input, it uses the representation of a specific token index.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Class Parameters**:\n",
        "  - `model`: The model whose outputs need to be pooled.\n",
        "  - `pooling_strategy`: Strategy for pooling, can be one of several predefined options or a specific token index. Default is 'cls'.\n",
        "  - `padding_strategy`: Strategy for padding, can be 'left' or 'right'. Default is 'left'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyFWXUQXhWHY"
      },
      "outputs": [],
      "source": [
        "class Pooler:\n",
        "    def __init__(self,\n",
        "                model,\n",
        "                padding_strategy: Optional[str] = 'left'):\n",
        "        self.model = model\n",
        "        self.padding_strategy = padding_strategy\n",
        "\n",
        "    def __call__(self, inputs, layer_index=-1) -> Any:\n",
        "        all_layer_outputs = self.model(output_hidden_states=True, return_dict=True, **inputs).hidden_states\n",
        "        outputs = all_layer_outputs[layer_index]\n",
        "        batch_size = inputs['input_ids'].shape[0]\n",
        "        sequence_lengths = -1 if self.padding_strategy == 'left' else inputs[\"attention_mask\"].sum(dim=1) - 1\n",
        "        outputs = outputs[torch.arange(batch_size, device=outputs.device), sequence_lengths]\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVDqc2HoKXpU"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA8qbPXoUaFl"
      },
      "source": [
        "The custom trainer method extends the `Trainer` method of `HuggingFace Transformers`. It is used to override the `compute_loss` function and include our custom losses for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RpRE7z8hWHY"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, pooler: Pooler, loss_kwargs: Optional[Dict] = None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pooler = pooler\n",
        "        if loss_kwargs is None:\n",
        "            loss_kwargs = {}\n",
        "        self.loss_fct = TotalLoss(**loss_kwargs)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\", None)\n",
        "        outputs = self.pooler(inputs)\n",
        "        loss = self.loss_fct(labels, outputs)\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDsjQhxkKq1Y"
      },
      "source": [
        "## Fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajV4GQVDUaFm"
      },
      "source": [
        "The `fit` function trains a model using a custom trainer and pooling mechanism. It initializes and configures the training process, including the dataset, model, tokenizer, and various training arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_41sJ58WhWHY"
      },
      "outputs": [],
      "source": [
        "def fit(train_ds,\n",
        "        model_base,\n",
        "        tokenizer_base,\n",
        "        batch_size: int = 32,\n",
        "        output_dir: Optional[str] = 'chk/new_c',\n",
        "        epochs: int = 5,\n",
        "        learning_rate: float = 1e-5,\n",
        "        warmup_steps: int = 1000,\n",
        "        logging_steps: int = 10,\n",
        "        eval_steps: Optional[int] = None,\n",
        "        save_steps: int = 100,\n",
        "        save_strategy: str = 'steps',\n",
        "        save_total_limit: int = 10,\n",
        "        gradient_accumulation_steps: int = 1,\n",
        "        fp16: Optional[bool] = None,\n",
        "        argument_kwargs: Optional[Dict] = None,\n",
        "        trainer_kwargs: Optional[Dict] = None,\n",
        "        loss_kwargs: Optional[Dict] = None):\n",
        "\n",
        "    if argument_kwargs is None:\n",
        "        argument_kwargs = {}\n",
        "    if trainer_kwargs is None:\n",
        "        trainer_kwargs = {}\n",
        "    callbacks = None\n",
        "\n",
        "    pooler = Pooler(model_base)\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        pooler=pooler,\n",
        "        model=model_base,\n",
        "        train_dataset=train_ds,\n",
        "        loss_kwargs=loss_kwargs,\n",
        "        tokenizer=tokenizer_base,\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            warmup_steps=warmup_steps,\n",
        "            num_train_epochs=epochs,\n",
        "            learning_rate=learning_rate,\n",
        "            fp16=fp16,\n",
        "            logging_steps=logging_steps,\n",
        "            save_strategy=save_strategy,\n",
        "            eval_steps=eval_steps,\n",
        "            save_steps=save_steps,\n",
        "            output_dir=output_dir,\n",
        "            save_total_limit=save_total_limit,\n",
        "            load_best_model_at_end=False,\n",
        "            ddp_find_unused_parameters=None,\n",
        "            label_names=['labels', 'seperate_ids', 'extra'],\n",
        "            **argument_kwargs,\n",
        "        ),\n",
        "        callbacks=callbacks,\n",
        "        data_collator=CustomDataCollator(\n",
        "            tokenizer_base,\n",
        "            max_length=1024\n",
        "        ),\n",
        "        **trainer_kwargs\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model_base, tokenizer_base, pooler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DHY-LVnFSga"
      },
      "source": [
        "## LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Frxf8nrFUKS"
      },
      "outputs": [],
      "source": [
        "lora_config_obj = {\n",
        "    \"lora_r\": 88,\n",
        "    \"lora_alpha\": 16,\n",
        "    \"lora_dropout\": 0.1\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeYYm27IFXHS"
      },
      "outputs": [],
      "source": [
        "def get_peft_config(lora_config_obj):\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha = lora_config_obj['lora_alpha'],\n",
        "        lora_dropout = lora_config_obj['lora_dropout'],\n",
        "        r = lora_config_obj['lora_r'],\n",
        "        bias = \"none\",\n",
        "        task_type = TaskType.FEATURE_EXTRACTION,\n",
        "    )\n",
        "    return peft_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpiS9-RxKuun"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CovZGEUAUaFm"
      },
      "source": [
        "The `encode` function processes input text using a specified model, applies a specified pooling strategy, and converts the output to a NumPy array or in other words generates **embeddings** of text, which are fixed-size representations suitable for downstream tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SASA4fFghWHZ"
      },
      "outputs": [],
      "source": [
        "def encode(inputs: Union[List[str], Tuple[str], List[Dict], str],\n",
        "            model,\n",
        "            pooler,\n",
        "            tokenizer,\n",
        "            max_length: Optional[int] = 1024,\n",
        "            to_numpy: bool = True,\n",
        "            device: Optional[Any] = 'cuda:0'):\n",
        "        if device is None:\n",
        "            device = 'cpu'\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        tok = tokenizer(\n",
        "            inputs,\n",
        "            padding='longest',\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            return_tensors='pt')\n",
        "        tok.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = pooler(tok)\n",
        "        if to_numpy:\n",
        "            return output.float().detach().cpu().numpy()\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNQpxXtVHN8M"
      },
      "source": [
        "## Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhlgulCwHVGv",
        "outputId": "7511a646-1a80-43b4-9f2b-09437ea4e84c",
        "colab": {
          "referenced_widgets": [
            "8846470f184e4a519ea0e4d25345ded0"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8846470f184e4a519ea0e4d25345ded0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer_base = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\", trust_remote_code=True)\n",
        "tokenizer_base.pad_token = tokenizer_base.eos_token\n",
        "tokenizer_base.padding_side = \"left\"\n",
        "tokenizer_base.padding_value = 0\n",
        "tokenizer_base.pad_token_id = 0\n",
        "\n",
        "model_base = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\", device_map=\"auto\")\n",
        "model_base.config.use_cache = False\n",
        "model_base.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ozs6-khIa9N"
      },
      "outputs": [],
      "source": [
        "model_base = get_peft_model(model_base, get_peft_config(lora_config_obj)) # Using LoRA..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj08gucMHVsL"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya__1EagHrms"
      },
      "outputs": [],
      "source": [
        "def get_senteval_dataset(dataset_name):\n",
        "    match dataset_name:\n",
        "        case 'CR':\n",
        "            dataset = load_dataset('rahulsikder223/SentEval-CR')\n",
        "        case 'MPQA':\n",
        "            dataset = load_dataset('rahulsikder223/SentEval-MPQA')\n",
        "        case 'MR':\n",
        "            dataset = load_dataset('rahulsikder223/SentEval-MR')\n",
        "\n",
        "            # We exclude this sentence since it gives null error...\n",
        "            dataset = concatenate_datasets([dataset.select(range(0, 231)), dataset.select(range(233, 7463))])\n",
        "        case 'SUBJ':\n",
        "            dataset = load_dataset('rahulsikder223/SentEval-SUBJ')\n",
        "\n",
        "    # We include a duplicate column 'text2' to make it work with the loss function defition...\n",
        "    dataset = dataset.rename_column('sentence', 'text1')\n",
        "    dataset['train'] = dataset['train'].add_column(\"text2\", dataset['train']['text1'])\n",
        "    dataset['test'] = dataset['test'].add_column(\"text2\", dataset['test']['text1'])\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVeTsONwHzQr"
      },
      "outputs": [],
      "source": [
        "senteval_datasets = ['CR']#['CR', 'MPQA', 'MR', 'SUBJ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv-ypVAHIjdF",
        "outputId": "b75a174b-9d49-47e5-9ea3-987dd0640ec2",
        "colab": {
          "referenced_widgets": [
            "e9cf50e8152840f5b267050ad5ecdb84",
            "bde91303ef914fb5bfb7f6b4488e8a06",
            "2e25f7b5868f458f829017524d0167ab",
            "a7fb08519f2d46f4a6be86c759492128",
            "9b364cbe544947b3843ec1bee07c1b72"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9cf50e8152840f5b267050ad5ecdb84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/639 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bde91303ef914fb5bfb7f6b4488e8a06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/171k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e25f7b5868f458f829017524d0167ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/75.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7fb08519f2d46f4a6be86c759492128",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/2642 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b364cbe544947b3843ec1bee07c1b72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/1133 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ds = get_senteval_dataset(senteval_datasets[0])\n",
        "ds_train = ds['train']\n",
        "ds_test = ds['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTPUuL8vIAX3"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJc_dXj_IYVU",
        "outputId": "b88af829-8a8f-4f3e-a4ca-be4904fcf794",
        "colab": {
          "referenced_widgets": [
            "0a898f7f9f77450eb08b53192b940bcb"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a898f7f9f77450eb08b53192b940bcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=8):   0%|          | 0/2642 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_ds = ds_train.shuffle().map(CustomDataTokenizer(tokenizer_base), num_proc=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im75ZUI9IBq0",
        "outputId": "293ce96f-dbef-44bd-d83d-4d0e45691adb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2645' max='2645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2645/2645 23:11, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_new, tokenizer_new, pooler_new = fit(\n",
        "    train_ds=train_ds,\n",
        "    model_base=model_base,\n",
        "    tokenizer_base=tokenizer_base,\n",
        "    output_dir='chk/c',\n",
        "    batch_size=5,\n",
        "    epochs=5,\n",
        "    learning_rate=2e-5,\n",
        "    save_steps=0,\n",
        "    eval_steps=10000,\n",
        "    warmup_steps=0,\n",
        "    gradient_accumulation_steps=1,\n",
        "    loss_kwargs={\n",
        "        'w_cosent': 1,\n",
        "        'w_ibn': 1,\n",
        "        'w_angle': 1,\n",
        "        'cosent_tau': 20,\n",
        "        'ibn_tau': 20,\n",
        "        'angle_tau': 1.0\n",
        "    },\n",
        "    fp16=False,\n",
        "    logging_steps=10000\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNznxtPsIS58"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVb2CmpVIUUE"
      },
      "outputs": [],
      "source": [
        "emb_train = []\n",
        "for sentence in ds_train['text1']:\n",
        "    emb_train.append(encode(sentence, model_new, pooler_new, tokenizer_new)[0])\n",
        "\n",
        "emb_test = []\n",
        "for sentence in ds_test['text1']:\n",
        "    emb_test.append(encode(sentence, model_new, pooler_new, tokenizer_new)[0])\n",
        "\n",
        "# Conversion into Numpy Array...\n",
        "emb_train = np.array(emb_train)\n",
        "emb_test = np.array(emb_test)\n",
        "\n",
        "# Classification...\n",
        "lr = LogisticRegression(max_iter=10000)\n",
        "lr.fit(emb_train, ds_train['label'])\n",
        "accuracy_score = lr.score(emb_test, ds_test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzEdwdVaI1Ks",
        "outputId": "0a3ade1e-2113-4606-a892-66cd6ec96e99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8870255957634599"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8HauEi1vUr_l",
        "FWJYPqj-JTYq",
        "oKd63ogoJZYp",
        "yDR2QBJiKnBf",
        "Wf43nyBfJpUi",
        "OkSrRaOJUaFj",
        "7gf8LPyIiW4Y",
        "4kVKFuNSh8ri",
        "KzGhQzFHiEow",
        "I2Xwuv6RUaFk",
        "bXi45wR2KFFv",
        "yVDqc2HoKXpU",
        "kDsjQhxkKq1Y",
        "6DHY-LVnFSga",
        "OpiS9-RxKuun",
        "UNQpxXtVHN8M",
        "Dj08gucMHVsL",
        "aTPUuL8vIAX3",
        "RNznxtPsIS58"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}